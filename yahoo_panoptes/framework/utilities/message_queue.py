"""
Copyright 2018, Oath Inc.
Licensed under the terms of the Apache 2.0 license. See LICENSE file in project root for terms.

This module implements an abstract Message Producer based on Kafka Queues
"""
from kafka import KafkaProducer

from yahoo_panoptes.framework.exceptions import PanoptesBaseException
from yahoo_panoptes.framework.validators import PanoptesValidators
from yahoo_panoptes.framework.utilities.helpers import get_client_id

_PRODUCER_ID_PREFIX = 'this is a placeholder'


class PanoptesProducerError(PanoptesBaseException):
    """
    A class that encapsulates all context creation errors
    """
    pass


class PanoptesMessageQueueProducer:

    def __init__(self, panoptes_context, acks='all', compression_type=None, batch_size=16384, linger_ms=0, timeout=5):
        """
        https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html

        Args:
            panoptes_context (PanoptesContext): The PanoptesContext to be used by the consumer
            acks (str / int): The number of acknowledgments the producer requires the leader to have received before
                considering a request complete. This controls durability of the records sent.
                0     (int) - Producer will not wait for any acknowledgment from the server
                1     (int) - Wait for leader to write the record to its local log only
                'all' (str) - Wait for the full set of in-sync replicas to write the record
            compression_type (str) - The compression type for all data generated by the producer
            batch_size (int) - Controls how many bytes of data to collect before sending messages to Kafka
            linger_ms (int) - How long to wait for more messages to batch together before sending the data to Kafka.
                Increasing linger_ms will theoretically reduce the number of requests sent to kafka.
            timeout (int) - Timeout in seconds to wait for completion

        # Note partitioner(key_bytes, all_partitions, available_partitions) - default is murmur2
        """
        # assert isinstance(panoptes_context, PanoptesContext), 'panoptes_context must be an instance of PanoptesContext'
        assert acks in ['all', 0, 1], 'PanoptesProducer::acks must be set to `all`, 0, or 1. {} is invalid'.format(acks)
        assert compression_type in [None, 'gzip', 'snappy', 'lz4'], 'compression_type must be in ' \
                                                                    '[None, "gzip","snappy", "lz4"]. {} is invalid'\
                                                                    .format(compression_type)
        assert PanoptesValidators.valid_positive_integer(batch_size), 'batch_size must be a valid positive integer'
        assert PanoptesValidators.valid_positive_integer(linger_ms), 'linger_ms must be a valid positive integer'
        assert PanoptesValidators.valid_positive_integer(timeout), 'timeout must be a valid positive integer'

        self._acks = acks
        self._compression_type = compression_type
        self._batch_size = batch_size
        self._linger_ms = linger_ms
        self._timeout = timeout

        try:

            self._kafka_producer = KafkaProducer(bootstrap_servers=panoptes_context.config_object.kafka_brokers,
                                                 client_id=get_client_id(_PRODUCER_ID_PREFIX),
                                                 acks=self._acks,
                                                 compression_type=self._compression_type,
                                                 batch_size=self._batch_size,
                                                 linger_ms=self._linger_ms)
        except Exception as e:
            raise PanoptesProducerError('Could not connect to any Kafka broker from this list: {} - {}'
                                        .format(panoptes_context.config_object.kafka_brokers, e))

        if not self._kafka_producer.bootstrap_connected():
            raise PanoptesProducerError('Could not connect to any Kafka broker from this list: {}'
                                        .format(panoptes_context.config_object.kafka_brokers))

    def __del__(self):
        if hasattr(self, '_kafka_producer'):
            self._kafka_producer.close(timeout=self._timeout)

    def send_messages(self, topic, key, messages, partitioning_key=None):
        """
                Send messages to the specified topic

                This method tries to ensures that the topic exists before trying to send a message to it. If auto-creation of
                topics is enabled, then this should always succeed (barring Kafka/Zookeeper failures)

                Args:
                    topic (str): The topic to which the message should be sent
                    key (str): The key for the message
                    messages (str): The message to send
                    partitioning_key (str): If provided, then it would be used to select the partition for the message instead \
                    of the message key

                Returns:
                    None: Nothing. Passes through exceptions in case of failure

                """
        assert PanoptesValidators.valid_nonempty_string(topic), u'topic must be a non-empty string'
        assert PanoptesValidators.valid_nonempty_string(key), u'key must be a non-empty string'
        assert PanoptesValidators.valid_nonempty_string(messages), u'messages must be a non-empty string'

        # kafka_producer.send() calls wait_on_metadata() which ensures that the topic exists.

        if partitioning_key:

            # We do this hack so that the partitioning key can be different from the message key
            # The function arguments key, value and serialized_value are all unused in kafka-python==1.4.7.
            partition = self._kafka_producer._partition(topic=topic,
                                                        partition=None,
                                                        key=key.encode('utf-8'),                        # unused
                                                        value=messages.encode('utf-8'),                 # unused
                                                        serialized_key=partitioning_key.encode('utf-8'),
                                                        serialized_value=messages.encode('utf-8'))      # unused

            self._kafka_producer.send(
                topic=topic,
                value=messages.encode('utf-8'),
                key=key.encode('utf-8'),
                partition=partition
            )

        else:
            # In this case, the message key is used as the partitioning key
            self._kafka_producer.send(
                topic=topic,
                key=key.encode('utf-8'),
                value=messages.encode('utf-8')
            )

    def stop(self):
        """
        Stop the message producer

        This method should always be called as part of cleanups so that any pending messages (for async message \
        producers) can be flushed and the Kafka connection closed cleanly

        Returns:
            None: Nothing. Passes through exceptions in case of failure

        """
        if hasattr(self, '_kafka_producer') and not self._kafka_producer._closed:
            self._kafka_producer.close(timeout=self._timeout)
